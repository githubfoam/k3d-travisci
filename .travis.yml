---
sudo: required
dist: bionic
notifications:
  slack:
    on_failure: always
# fleet_k3d_singlemaster_tasks : &fleet_k3d_singlemaster_tasks
#       script:
#         - sudo sh -c "curl -s https://raw.githubusercontent.com/rancher/k3d/master/install.sh | bash" #grab the latest release
#         - k3d version
#         - k3d check-tools #Check if docker is running
#         - sudo k3d --timestamp --verbose  create cluster --wait 360 --name demo --api-port localhost:6444 --publish 8080:80 --server-arg --tls-san="127.0.0.1"  --workers 3 #Create a single- or multi-node k3s cluster in docker containers
#         - docker ps
#         - k3d list clusters
#         - export KUBECONFIG="$(sudo k3d get-kubeconfig --name='demo')"
#         - cat $KUBECONFIG
#         - sudo kubectl get nodes
#         - sudo kubectl cluster-info
#         - sudo kubectl get pods --all-namespaces
#         #Add local storage
#         - sudo kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
#         - kubectl get storageclass

fleet_k3d_singlemaster_tasks : &fleet_k3d_singlemaster_tasks
      script:
        - sudo sh -c "curl -s https://raw.githubusercontent.com/rancher/k3d/master/install.sh | bash" #grab the latest release
        - k3d version
        - k3d check-tools #Check if docker is running
        - sudo k3d --timestamp --verbose  create cluster --wait 360 --name demo --workers 4
        - docker ps
        - k3d list clusters
        - export KUBECONFIG="$(sudo k3d get-kubeconfig --name='demo')"
        - cat $KUBECONFIG
        - sudo kubectl get nodes
        - sudo kubectl get pod -o wide
        - sudo kubectl get pod -n kube-system -o wide
        - sudo kubectl get pod -n default -o wide
        - sudo kubectl get pods --all-namespaces
        - sudo kubectl get service --all-namespaces
        - sudo kubectl cluster-info
        # curl: (60) SSL certificate problem: unable to get local issuer certificate
        # - |
        #   url="https://localhost:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy"
        #   if curl --output /dev/null --silent --head --fail "$url"; then
        #   echo "Metrics-server is running at $url"
        #   else
        #   echo "URL does not exist: $url"
        #   fi
        - |
          for i in {1..60}; do # Timeout after 5 minutes, 60x1=60 secs
              if nc -z -v 127.0.0.1 6443 2>&1 | grep succeeded ; then
                break
              fi
              sleep 5
          done
        - kubectl get nodes --all-namespaces # verify "*helm-install-traefik-*"
        - |
          for i in {1..60}; do # Timeout after 5 minutes, 60x7= 7 mins
              if [[ $(sudo kubectl get nodes | awk '{print $1}') == "*helm-install-traefik-*" ]] && [[ $(sudo kubectl get nodes | awk '{print $4}') == "Running" ]]; then
                break
              fi
              sleep 3
          done
        - kubectl get nodes --all-namespaces # verify "*helm-install-traefik-*"
        - kubectl get pods --namespace=kube-system # verify namespace=kube-system Running
        - |
          echo "Waiting for Kubernetes to be ready ..."
          for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              if sudo kubectl get pods --namespace=kube-system | grep Running ; then
                break
              fi
              sleep 2
          done
        - kubectl get pods --namespace=kube-system # verify namespace=kube-system Running
        - sudo kubectl get nodes
        - sudo kubectl get pod -o wide
        - sudo kubectl get pod -n kube-system -o wide
        - sudo kubectl get pod -n default -o wide
        - sudo kubectl get pods --all-namespaces
        - sudo kubectl get service --all-namespaces
        - sudo kubectl cluster-info
        ###################### #Add local storage ######################
        - sudo kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
        - sudo kubectl get pods --all-namespaces
        - sudo kubectl get storageclass
        - |
          echo "Waiting for local storage to be ready ..."
          for i in {1..60}; do # Timeout after 5 minutes, 150x5=300 secs
              if sudo kubectl get pods --namespace=local-path-storage | grep Running ; then
                break
              fi
              sleep 5
          done
        - sudo kubectl get pods --all-namespaces
        - sudo kubectl get pods --namespace=local-path-storage
        -  kubectl get pods --all-namespaces # verify "*local-path-storage*" Running
        - |
          echo "Waiting for Kubernetes to be ready ..."
          for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              if [[ $(sudo kubectl get pods --all-namespaces | awk '{print $1}') == "*local-path-storage*" ]] && [[ $(sudo kubectl get pods --all-namespaces | awk '{print $2}') == "Running" ]] ; then
                break
              fi
              sleep 2
          done
        -  kubectl get pods --all-namespaces # verify "*local-path-storage*" Running
        ###################### #Add local storage ######################
        - echo "###################### #openEBS ################################"
        - sudo kubectl apply -f https://openebs.github.io/charts/openebs-operator.yaml #install OpenEBS
        - kubectl get service --all-namespaces # find a Service IP,list all services in all namespaces
        - kubectl get pods -n openebs -l openebs.io/component-name=openebs-localpv-provisioner #Observe localhost provisioner pod
        - kubectl get sc #Check the storage Class
        - |
          echo "Waiting for local storage to be ready ..."
          for i in {1..60}; do # Timeout after 5 minutes, 150x5=300 secs
              if sudo kubectl get pods --namespace=openebs -l openebs.io/component-name=openebs-localpv-provisioner | grep Running ; then
                break
              fi
              sleep 5
          done
        - |
          echo "Waiting for local storage to be ready ..."
          for i in {1..60}; do # Timeout after 2 minutes, 60x2=300 secs
                if sudo kubectl get pods --namespace=local-path-storage | grep Running ; then
                  break
                fi
                sleep 2
          done
        - sudo kubectl get pods --all-namespaces
        - sudo kubectl get pods --namespace=openebs
        - echo "###################### #openEBS ################################"
        ###################### #Local Docker Registry ##################
        - echo "127.0.0.1 registry.local" |sudo tee -a /etc/hosts
        - sudo cat /etc/hosts
        ###################### #Local Docker Registry ##################
        ###################### #flask-helm-example ##################
        # build and push docker image
        # - pushd $(pwd) && cd flask-helm-example
        # - docker build . -t flask-helm-example:latest --network host
        # - docker tag flask-helm-example:latest registry.local:5000/flask-helm-example
        # - sudo netstat -tlpen | grep 5000
        # - docker push registry.local:5000/flask-helm-example
        # # start the helm chart with local values
        # - helm install flask-helm-example -f devvalues.yaml ./chart
        # - sudo netstat -tlpen | grep 8080
        # - - curl http://127.0.0.1:8080/flask-helm-example
        # - curl http://localhost:8080/flask-helm-example
        # - |
        #   url="https://localhost:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy"
        #   if curl --output /dev/null --silent --head --fail "$url"; then
        #   echo "Metrics-server is running at $url"
        #   else
        #   echo "URL does not exist: $url"
        #   fi
        # - popd
        ###################### #flask-helm-example ##################
        # - helm uninstall <name>
        # - helm status <name>
        #Setting up Helm
        # - helm list
        # - sudo kubectl -n kube-system create serviceaccount tiller
        # - sudo kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
        # - sudo helm init --service-account tiller
        # - helm ls
        # - helm repo update
        # - helm search sonarqube
        # - helm ls
        # - sudo helm install stable/sonarqube --namespace sonar
        # - helm ls
        # - kubectl get pods -n sonarqube
        # - kubectl get events
        # # - kubectl exec -it sonar-sonarqube-6ff4554ff4-4f7p2 bash
        # - helm list
        ###################### app1 ######################
fleet_script_tasks : &fleet_script_tasks
      script:
        - python --version
fleet_install_tasks : &fleet_install_tasks
      install:
        - pip install -r requirements.txt
matrix:
  fast_finish: true
  include:
    - name: "app1 k3d single master  Python 3.7 on bionic amd64"
      os: linux
      dist: bionic
      arch: amd64
      addons:
        snaps:
          - name: multipass
            confinement: classic # or devmode
            channel: latest/stable # will be passed to --channel flag
          - name: kubectl
            confinement: classic # or devmode
            channel: latest/stable # will be passed to --channel flag
          - name: helm
            confinement: classic # or devmode
            channel: latest/stable # will be passed to --channel flag
      language: python
      python: 3.7
      before_install:
        - pip3 install virtualenv
        - virtualenv -p $(which python3) ~venvpy3
        - source ~venvpy3/bin/activate
      <<: *fleet_install_tasks
      <<: *fleet_script_tasks
      <<: *fleet_k3d_singlemaster_tasks
      # <<: *fleet_app1_tasks
      after_success:
        - deactivate
    # - name: "k3d single master  Python 3.7 on bionic amd64"
    #   os: linux
    #   dist: bionic
    #   arch: amd64
    #   addons:
    #     snaps:
    #       - name: multipass
    #         confinement: classic # or devmode
    #         channel: latest/stable # will be passed to --channel flag
    #       - name: kubectl
    #         confinement: classic # or devmode
    #         channel: latest/stable # will be passed to --channel flag
    #   language: python
    #   python: 3.7
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_k3d_singlemaster_tasks
    #   after_success:
    #     - deactivate
    # - name: "k3d single master  Python 3.7 on xenial amd64"
    #   os: linux
    #   dist: xenial
    #   arch: amd64
    #   addons:
    #     snaps:
    #       - name: multipass
    #         confinement: classic # or devmode
    #         channel: latest/stable # will be passed to --channel flag
    #       - name: kubectl
    #         confinement: classic # or devmode
    #         channel: latest/stable # will be passed to --channel flag
    #   language: python
    #   python: 3.7
    #   before_install:
    #     - pip3 install virtualenv
    #     - virtualenv -p $(which python3) ~venvpy3
    #     - source ~venvpy3/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   <<: *fleet_k3d_singlemaster_tasks
    #   after_success:
    #     - deactivate
# #============================================================================ multiarch linux ============================================================================
#     - name: "k3d single master  Python 3.7 on bionic arm64"
#       os: linux
#       dist: bionic
#       arch: arm64
#       addons:
#         snaps:
#           - name: multipass
#             confinement: classic # or devmode
#             channel: latest/beta # will be passed to --channel flag
#           - name: kubectl
#             confinement: classic # or devmode
#             channel: latest/stable # will be passed to --channel flag
#       language: python
#       python: 3.7
#       before_install:
#         - pip3 install virtualenv
#         - virtualenv -p $(which python3) ~venvpy3
#         - source ~venvpy3/bin/activate
#       <<: *fleet_install_tasks
#       <<: *fleet_script_tasks
#       # <<: *fleet_k3d_singlemaster_tasks
#       script:
#         - sudo sh -c "curl -s https://raw.githubusercontent.com/rancher/k3d/master/install.sh | bash" #grab the latest release
#         - k3d version
#         - k3d check-tools #Check if docker is running
#         - sudo k3d --timestamp --verbose  create cluster --wait 360 --name demo --api-port localhost:6444 --publish 8080:80 --server-arg --tls-san="127.0.0.1"  --workers 3 #Create a single- or multi-node k3s cluster in docker containers
#         - docker ps
#         - k3d list clusters
#         - export KUBECONFIG="$(sudo k3d get-kubeconfig --name='demo')"
#         # - cat $KUBECONFIG
#         # - sudo kubectl get nodes
#         # - sudo kubectl cluster-info
#       after_success:
#         - deactivate
#     - name: "k3d single master  Python 3.7 on xenial arm64"
#       os: linux
#       dist: xenial
#       arch: arm64
#       addons:
#         snaps:
#           - name: multipass
#             confinement: classic # or devmode
#             channel: latest/edge # will be passed to --channel flag
#           - name: kubectl
#             confinement: classic # or devmode
#             channel: latest/stable # will be passed to --channel flag
#       language: python
#       python: 3.7
#       before_install:
#         - pip3 install virtualenv
#         - virtualenv -p $(which python3) ~venvpy3
#         - source ~venvpy3/bin/activate
#       <<: *fleet_install_tasks
#       <<: *fleet_script_tasks
#       # <<: *fleet_k3d_singlemaster_tasks
#       script:
#         - sudo sh -c "curl -s https://raw.githubusercontent.com/rancher/k3d/master/install.sh | bash" #grab the latest release
#         - k3d version
#         - k3d check-tools #Check if docker is running
#         # - sudo k3d --timestamp --verbose  create cluster --wait 360 --name demo --api-port localhost:6444 --publish 8080:80 --server-arg --tls-san="127.0.0.1"  --workers 3 #Create a single- or multi-node k3s cluster in docker containers
#         # - docker ps
#         # - k3d list clusters
#         # - export KUBECONFIG="$(sudo k3d get-kubeconfig --name='demo')"
#         # - cat $KUBECONFIG
#         # - sudo kubectl get nodes
#         # - sudo kubectl cluster-info
#       after_success:
#         - deactivate
#     - name: "k3d single master Python 3.7 on bionic ppc64le" #No prebuilt binary for linux-ppc64le.
#       os: linux
#       arch: ppc64le
#       dist: bionic
#       language: python
#       python: 3.7
#       before_install:
#         - pip3 install virtualenv
#         - virtualenv -p $(which python3) ~venvpy3
#         - source ~venvpy3/bin/activate
#       <<: *fleet_install_tasks
#       <<: *fleet_script_tasks
#       # <<: *fleet_k3d_singlemaster_tasks
#       after_success:
#         - deactivate
#     - name: "k3d single master Python 3.7 on bionic s390x" #No prebuilt binary for linux-s390x
#       os: linux
#       arch: s390x
#       dist: bionic
#       language: python
#       python: 3.7
#       before_install:
#         - pip3 install virtualenv
#         - virtualenv -p $(which python3) ~venvpy3
#         - source ~venvpy3/bin/activate
#       <<: *fleet_install_tasks
#       <<: *fleet_script_tasks
#       # <<: *fleet_k3d_singlemaster_tasks
#       after_success:
#         - deactivate
#     - name: "k3d single master Python 3.7 on xenial ppc64le" #No prebuilt binary for linux-ppc64le
#       os: linux
#       arch: ppc64le
#       dist: xenial
#       language: python
#       python: 3.7
#       before_install:
#         - pip3 install virtualenv
#         - virtualenv -p $(which python3) ~venvpy3
#         - source ~venvpy3/bin/activate
#       <<: *fleet_install_tasks
#       <<: *fleet_script_tasks
#       # <<: *fleet_k3d_singlemaster_tasks
#       after_success:
#         - deactivate
#     - name: "k3d single master Python 3.7 on xenial s390x" #No prebuilt binary for linux-s390x
#       os: linux
#       arch: s390x
#       dist: xenial
#       language: python
#       python: 3.7
#       before_install:
#         - pip3 install virtualenv
#         - virtualenv -p $(which python3) ~venvpy3
#         - source ~venvpy3/bin/activate
#       <<: *fleet_install_tasks
#       <<: *fleet_script_tasks
#       # <<: *fleet_k3d_singlemaster_tasks
#       after_success:
#         - deactivate
# # =============================================macOS=============================================
#     #The default backend on macOS is hyperkit, wrapping Apple’s Hypervisor.framework
#     #You need macOS Yosemite, version 10.10.3 or later installed on a 2010 or newer Mac
#     #https://docs.travis-ci.com/user/reference/osx/#macos-version
#     # https://multipass.run/docs/installing-on-macos
#     # https://brew.sh/
#     - name: "multipass on macOS 10.15.4 osx xcode11.5"
#       os: osx
#       osx_image: #installer: Error - Your CPU does not have the features necessary for Multipass. Installation cannot proceed
#         - xcode11.5
#       language: shell
#       addons:
#         homebrew:
#           packages:
#           - kubectl
#           - kubernetes-cli
#           - helm
#           update: true
#       before_install:
#         - pip install virtualenv
#         - virtualenv -p $(which python2) ~venvpy2
#         - source ~venvpy2/bin/activate
#       <<: *fleet_install_tasks
#       <<: *fleet_script_tasks
#       script:
#         # - brew cask install multipass
#         # - multipass version
#         - brew list --versions
#         - kubectl version --client
#       after_success:
#         # - brew cask uninstall multipass
#         - deactivate
# # # #   # =============================================windows=============================================
#       #Multipass defaults to using Hyper-V as it’s virtualization provider
#       # https://multipass.run/docs/installing-on-windows
#       #https://github.com/canonical/multipass/releases
#       #https://github.com/canonical/multipass
#     - name: "multipass  Python 3.8 on Windows"
#       os: windows
#       language: shell
#       env:
#         - PATH=/c/Python38:/c/Python38/Scripts:$PATH
#       before_install:
#         - choco install python --version 3.8.1
#         - pip install virtualenv
#         - virtualenv $HOME/venv
#         - source $HOME/venv/Scripts/activate
#       <<: *fleet_install_tasks
#       <<: *fleet_script_tasks
#       script:
#         - choco install kubernetes-helm
#         - choco install kubernetes-cli
#         - choco install wget
#         - wget https://github.com/canonical/multipass/releases/download/v1.2.1/multipass-1.2.1+win-win64.exe
#         - dir
#       after_success:
#         - deactivate
